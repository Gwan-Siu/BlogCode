{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, exctation maximization algorithm will be introduced. THe story will begin with an example of flipping coins and maximum likelihood estimator(MLE). This example is from the paper [what is the expectation maximization algorithm?](https://www.nature.com/articles/nbt1406)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Example of flipping coins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two coins A and B with unknown biases, $\\theta_{A}$ and $\\theta_{B}$ respectively. We do the experiemt that we fip the coins repeatedly. During the experiment, we keep track the record of the $x={x_{1},x_{2},...,x_{n}}$ and $z={z_{1},z_{2},...,z_{n}}$, where $x$ indicates the number of heads we observe when we fip the coin and $z$ imply the identity of coin we use to fip. Given $x$ and $z$, we want to estimated biases $\\theta$ of these two coins. Obviously, we can use MLE to estimate the biase $\\theta$ based on the bays theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation*}\n",
    "p(\\theta \\vert x, z)=\\frac{p(\\theta)p(x,z\\vert \\theta)}{p(x,z)}\n",
    "\\end{equation*}$$\n",
    "\n",
    "where we assume the normalization term and prior term are constant, thus to maximize the posterior is equivalent to maximize the likelihood $p(x,z\\vert \\theta)$. That's a **commom parameter estimation problem.** This kind of estimated problem is called parameter estimated with complete data.\n",
    "\n",
    "Now, we consider a more challenging variant problem that we still we want to estimate baises of two coins A and B, but in this time, we only has the record $x$ and do not know about the $z$, where $z$ is called hidden parameter or latent factor. Parameter estimated problem with complete data is converted into parameter estimated problem with imcomplete data, so the MLE is not effective for the problem of imcomplete data. This is a big limitation of MLE and thus we need another method to sovle for imcomplete data problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "However, if we have some way to know the value of $z$, we can use MLE to inference $\\theta$ which is the same as parameter estimation with complete data. Thus, we can reduce parameter estimation for this problem with incomplete data to maximum likelihood estimation with complete data.\n",
    "\n",
    "One iterative scheme for obtaining completions caould works as follows: starting from some initial parameters, $\\widetilde{\\theta}^{(t)}=(\\widetilde{\\theta}_{A}^{(t)},\\widetilde{\\theta}_{B}^{(t)})$, determine for each of the five sets whether coin $A$ or coin $B$ was more liekly to have generated the observed flips(using the current parameter estimates). Then, assume these completions (that is , guessed coin assignments) to be correct, and apply the regular maximum likelihood estimation procedure to get $\\widetilde{\\theta}^{(t+1)}$. Finally, repeat these two steps until convergence. \n",
    "\n",
    "The expectation maximization algorithm is a refinement on this basic idea that the expectation maximization algorithm computes probabilities for each possible completion of the missing data, using the current parameters $\\widetilde{\\theta}^{(t)}$. These probabilities are used to create a weighted training set consisting of all possible completions of the data. Then, a modified version of maximum likelihood estimation that deals with weighted training examples provides new parameter estimates, $\\widetilde{\\theta}^{(t+1)}$. The figure below show the procedure of maximum likelihood estimator and expectation maximization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/EM_and_VI/image/MlE_EM.png\" width = \"600\" height = \"400\" alt=\"Inverse Transform\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EM_algorithm](https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/EM_and_VI/image/MlE_EM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
